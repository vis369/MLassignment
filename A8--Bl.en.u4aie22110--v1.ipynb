{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A1 Decission tree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        \n",
    "    def entropy(self, labels):\n",
    "        \"\"\"\n",
    "        Calculate entropy given a list of labels.\n",
    "        \"\"\"\n",
    "        unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = label_counts / len(labels)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, data, feature_values, target_values):\n",
    "        \"\"\"\n",
    "        Calculate information gain for a specific feature.\n",
    "        \"\"\"\n",
    "        # Calculate entropy of the whole dataset\n",
    "        total_entropy = self.entropy(target_values)\n",
    "\n",
    "        # Calculate weighted average entropy after splitting on the given feature\n",
    "        weighted_entropy = 0\n",
    "        for value in set(feature_values):\n",
    "            subset_indices = np.where(feature_values == value)[0]  # Extract the first element of the tuple\n",
    "            subset_targets = target_values[subset_indices]\n",
    "            weighted_entropy += len(subset_targets) / len(target_values) * self.entropy(subset_targets)\n",
    "\n",
    "        # Calculate information gain\n",
    "        info_gain = total_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def root_node_feature(self, features, target_values):\n",
    "        \"\"\"\n",
    "        Find the feature with the highest information gain to use as the root node.\n",
    "        \"\"\"\n",
    "        max_info_gain = -1\n",
    "        root_feature = None\n",
    "        for feature in features.columns:\n",
    "            gain = self.information_gain(features[feature], features[feature], target_values)  # Pass each feature separately\n",
    "            if gain > max_info_gain:\n",
    "                max_info_gain = gain\n",
    "                root_feature = feature\n",
    "        return root_feature\n",
    "\n",
    "    def load_data(self, excel_path, binning_type=None, num_bins=None):\n",
    "        df = pd.read_excel(excel_path)\n",
    "        features = df.drop('Disease', axis=1)\n",
    "        labels = df['Disease']\n",
    "        \n",
    "        # Check if binning is required\n",
    "        if binning_type and num_bins:\n",
    "            for col in features.columns:\n",
    "                if features[col].dtype != 'object':  # Check if the feature is continuous\n",
    "                    bins = self.binning(features[col], num_bins, binning_type)\n",
    "                    features[col] = pd.cut(features[col], bins=bins, labels=False)\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "    def is_pure(self, s):\n",
    "        return len(set(s)) == 1\n",
    "\n",
    "    def most_common(self, a):\n",
    "        (values, counts) = np.unique(a, return_counts=True)\n",
    "        ind = np.argmax(counts)\n",
    "        return values[ind]\n",
    "\n",
    "    def recursive_split(self, x, y):\n",
    "        if self.is_pure(y) or len(y) == 0:\n",
    "            return self.most_common(y)\n",
    "\n",
    "        gain = np.array([self.information_gain(y, x_attr, y) for x_attr in x.T])\n",
    "\n",
    "        if np.all(gain < 1e-6):\n",
    "            return self.most_common(y)\n",
    "\n",
    "        selected_attr = np.argmax(gain)\n",
    "        sets = self.partition(x[:, selected_attr])\n",
    "\n",
    "        res = {}\n",
    "        for key, value in sets.items():\n",
    "            y_subset = y.take(value, axis=0)\n",
    "            x_subset = x.take(value, axis=0)\n",
    "            if len(y_subset) > 0:  # Check if there are samples left\n",
    "                res[\"x_%d = %s\" % (selected_attr, key)] = self.recursive_split(x_subset, y_subset)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def partition(self, a):\n",
    "        return {c: (a == c).nonzero()[0] for c in np.unique(a)}\n",
    "\n",
    "    def print_tree(self, d, depth=0):\n",
    "        for key, value in d.items():\n",
    "            for i in range(depth):\n",
    "                print(' ', end='')\n",
    "            if type(value) is dict:\n",
    "                print(key, end=':\\n')\n",
    "                self.print_tree(value, depth + 1)\n",
    "            else:\n",
    "                print(key, end=': ')\n",
    "                print(value)\n",
    "    \n",
    "    def build_tree(self, features, labels):\n",
    "        self.tree = self.recursive_split(features.to_numpy(), labels.to_numpy())\n",
    "        \n",
    "    def binning(self, data, num_bins, binning_type='equal_width'):\n",
    "        if binning_type == 'equal_width':\n",
    "            return np.linspace(data.min(), data.max(), num_bins + 1)\n",
    "        elif binning_type == 'frequency':\n",
    "            bins = pd.cut(data, bins=num_bins, duplicates='drop')\n",
    "            return sorted(bins.unique())\n",
    "        else:\n",
    "            raise ValueError(\"Invalid binning type. Choose 'equal_width' or 'frequency'.\")\n",
    "\n",
    "# Create an instance of the DecisionTree class\n",
    "dt = DecisionTree()\n",
    "\n",
    "# Load data with binning\n",
    "features, labels = dt.load_data(\"C:/Users/vishn/Downloads/extracted_features.xlsx\", binning_type='equal_width', num_bins=5)\n",
    "\n",
    "# Build decision tree\n",
    "dt.build_tree(features, labels)\n",
    "\n",
    "# Print the decision tree\n",
    "dt.print_tree(dt.tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dd1735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(labels):\n",
    "    \"\"\"\n",
    "    Calculate entropy given a list of labels.\n",
    "    \"\"\"\n",
    "    unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = label_counts / len(labels)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "def information_gain(data, feature_values, target_values):\n",
    "    \"\"\"\n",
    "    Calculate information gain for a specific feature.\n",
    "    \"\"\"\n",
    "    # Calculate entropy of the whole dataset\n",
    "    total_entropy = entropy(target_values)\n",
    "\n",
    "    # Calculate weighted average entropy after splitting on the given feature\n",
    "    weighted_entropy = 0\n",
    "    for value in set(feature_values):\n",
    "        subset_indices = np.where(feature_values == value)[0]  # Extract the first element of the tuple\n",
    "        subset_targets = target_values[subset_indices]\n",
    "        weighted_entropy += len(subset_targets) / len(target_values) * entropy(subset_targets)\n",
    "\n",
    "    # Calculate information gain\n",
    "    info_gain = total_entropy - weighted_entropy\n",
    "    return info_gain\n",
    "\n",
    "def root_node_feature(features, target_values):\n",
    "    \"\"\"\n",
    "    Find the feature with the highest information gain to use as the root node.\n",
    "    \"\"\"\n",
    "    max_info_gain = -1\n",
    "    root_feature = None\n",
    "    for feature in features.columns:\n",
    "        gain = information_gain(features[feature], features[feature], target_values)  # Pass each feature separately\n",
    "        if gain > max_info_gain:\n",
    "            max_info_gain = gain\n",
    "            root_feature = feature\n",
    "    return root_feature\n",
    "\n",
    "def load_data(excel_path, binning_type=None, num_bins=None):\n",
    "    df = pd.read_excel(excel_path)\n",
    "    features = df.drop('Disease', axis=1)\n",
    "    labels = df['Disease']\n",
    "    \n",
    "    # Check if binning is required\n",
    "    if binning_type and num_bins:\n",
    "        for col in features.columns:\n",
    "            if pd.api.types.is_numeric_dtype(features[col]):  # Check if the feature is numeric\n",
    "                bins = binning_equal_width(features[col], num_bins)\n",
    "                features[col] = pd.cut(features[col], bins=bins, labels=False)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def binning_equal_width(feature_values, num_bins):\n",
    "    min_val = feature_values.min()\n",
    "    max_val = feature_values.max()\n",
    "    bin_width = (max_val - min_val) / num_bins\n",
    "    bins = [min_val + i * bin_width for i in range(num_bins)]\n",
    "    bins.append(max_val)  # Add the upper bound of the last bin\n",
    "    return bins\n",
    "\n",
    "def is_pure(s):\n",
    "    return len(set(s)) == 1\n",
    "\n",
    "def most_common(a):\n",
    "    (values, counts) = np.unique(a, return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    return values[ind]\n",
    "\n",
    "def recursive_split(x, y):\n",
    "    if is_pure(y) or len(y) == 0:\n",
    "        return most_common(y)\n",
    "\n",
    "    gain = np.array([information_gain(y, x_attr, y) for x_attr in x.T])\n",
    "    \n",
    "    if np.all(gain < 1e-6):\n",
    "        return most_common(y)\n",
    "\n",
    "    selected_attr = np.argmax(gain)\n",
    "    sets = partition(x[:, selected_attr])\n",
    "\n",
    "    res = {}\n",
    "    for key, value in sets.items():\n",
    "        y_subset = y.take(value, axis=0)\n",
    "        x_subset = x.take(value, axis=0)\n",
    "        if len(y_subset) > 0:  # Check if there are samples left\n",
    "            res[\"x_%d = %s\" % (selected_attr, key)] = recursive_split(x_subset, y_subset)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def partition(a):\n",
    "    return {c: (a == c).nonzero()[0] for c in np.unique(a)}\n",
    "\n",
    "def print_tree(d, depth=0):\n",
    "    for key, value in d.items():\n",
    "        for i in range(depth):\n",
    "            print(' ', end='')\n",
    "        if type(value) is dict:\n",
    "            print(key, end=':\\n')\n",
    "            print_tree(value, depth + 1)\n",
    "        else:\n",
    "            print(key, end=': ')\n",
    "            print(value)\n",
    "\n",
    "# Load data with binning\n",
    "features, labels = load_data(\"C:/Users/vishn/Downloads/extracted_features.xlsx\", binning_type='equal_width', num_bins=5)\n",
    "\n",
    "# Detect root node feature\n",
    "root_feature = root_node_feature(features, labels)\n",
    "\n",
    "# Perform algorithm on the example dataset to create a decision tree\n",
    "d = recursive_split(features.to_numpy(), labels.to_numpy())\n",
    "\n",
    "# Print the decision tree\n",
    "print_tree(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f63d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A3  Expand the above functions to built your own Decision Tree module.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        \n",
    "    def entropy(self, labels):\n",
    "        \"\"\"\n",
    "        Calculate entropy given a list of labels.\n",
    "        \"\"\"\n",
    "        unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = label_counts / len(labels)\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy\n",
    "\n",
    "    def information_gain(self, data, feature_values, target_values):\n",
    "        \"\"\"\n",
    "        Calculate information gain for a specific feature.\n",
    "        \"\"\"\n",
    "        # Calculate entropy of the whole dataset\n",
    "        total_entropy = self.entropy(target_values)\n",
    "\n",
    "        # Calculate weighted average entropy after splitting on the given feature\n",
    "        weighted_entropy = 0\n",
    "        for value in set(feature_values):\n",
    "            subset_indices = np.where(feature_values == value)[0]  # Extract the first element of the tuple\n",
    "            subset_targets = target_values[subset_indices]\n",
    "            weighted_entropy += len(subset_targets) / len(target_values) * self.entropy(subset_targets)\n",
    "\n",
    "        # Calculate information gain\n",
    "        info_gain = total_entropy - weighted_entropy\n",
    "        return info_gain\n",
    "\n",
    "    def root_node_feature(self, features, target_values):\n",
    "        \"\"\"\n",
    "        Find the feature with the highest information gain to use as the root node.\n",
    "        \"\"\"\n",
    "        max_info_gain = -1\n",
    "        root_feature = None\n",
    "        for feature in features.columns:\n",
    "            gain = self.information_gain(features[feature], features[feature], target_values)  # Pass each feature separately\n",
    "            if gain > max_info_gain:\n",
    "                max_info_gain = gain\n",
    "                root_feature = feature\n",
    "        return root_feature\n",
    "\n",
    "    def load_data(self, excel_path, binning_type=None, num_bins=None):\n",
    "        df = pd.read_excel(excel_path)\n",
    "        features = df.drop('Disease', axis=1)\n",
    "        labels = df['Disease']\n",
    "        \n",
    "        # Check if binning is required\n",
    "        if binning_type and num_bins:\n",
    "            for col in features.columns:\n",
    "                if features[col].dtype != 'object':  # Check if the feature is continuous\n",
    "                    bins = self.binning(features[col], num_bins, binning_type)\n",
    "                    features[col] = pd.cut(features[col], bins=bins, labels=False)\n",
    "        \n",
    "        return features, labels\n",
    "\n",
    "    def is_pure(self, s):\n",
    "        return len(set(s)) == 1\n",
    "\n",
    "    def most_common(self, a):\n",
    "        (values, counts) = np.unique(a, return_counts=True)\n",
    "        ind = np.argmax(counts)\n",
    "        return values[ind]\n",
    "\n",
    "    def recursive_split(self, x, y):\n",
    "        if self.is_pure(y) or len(y) == 0:\n",
    "            return self.most_common(y)\n",
    "\n",
    "        gain = np.array([self.information_gain(y, x_attr, y) for x_attr in x.T])\n",
    "\n",
    "        if np.all(gain < 1e-6):\n",
    "            return self.most_common(y)\n",
    "\n",
    "        selected_attr = np.argmax(gain)\n",
    "        sets = self.partition(x[:, selected_attr])\n",
    "\n",
    "        res = {}\n",
    "        for key, value in sets.items():\n",
    "            y_subset = y.take(value, axis=0)\n",
    "            x_subset = x.take(value, axis=0)\n",
    "            if len(y_subset) > 0:  # Check if there are samples left\n",
    "                res[\"x_%d = %s\" % (selected_attr, key)] = self.recursive_split(x_subset, y_subset)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def partition(self, a):\n",
    "        return {c: (a == c).nonzero()[0] for c in np.unique(a)}\n",
    "\n",
    "    def print_tree(self, d, depth=0):\n",
    "        for key, value in d.items():\n",
    "            for i in range(depth):\n",
    "                print(' ', end='')\n",
    "            if type(value) is dict:\n",
    "                print(key, end=':\\n')\n",
    "                self.print_tree(value, depth + 1)\n",
    "            else:\n",
    "                print(key, end=': ')\n",
    "                print(value)\n",
    "    \n",
    "    def build_tree(self, features, labels):\n",
    "        self.tree = self.recursive_split(features.to_numpy(), labels.to_numpy())\n",
    "        \n",
    "    def binning(self, data, num_bins, binning_type='equal_width'):\n",
    "        if binning_type == 'equal_width':\n",
    "            return np.linspace(data.min(), data.max(), num_bins + 1)\n",
    "        elif binning_type == 'frequency':\n",
    "            bins = pd.cut(data, bins=num_bins, duplicates='drop')\n",
    "            return sorted(bins.unique())\n",
    "        else:\n",
    "            raise ValueError(\"Invalid binning type. Choose 'equal_width' or 'frequency'.\")\n",
    "\n",
    "# Create an instance of the DecisionTree class\n",
    "dt = DecisionTree()\n",
    "\n",
    "# Load data with binning\n",
    "features, labels = dt.load_data(\"C:/Users/vishn/Downloads/extracted_features.xlsx\", binning_type='equal_width', num_bins=5)\n",
    "\n",
    "# Build decision tree\n",
    "dt.build_tree(features, labels)\n",
    "\n",
    "# Print the decision tree\n",
    "dt.print_tree(dt.tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed518cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
